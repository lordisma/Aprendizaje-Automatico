<<<<<<< Updated upstream
setwd("E:/Projects/Aprendizaje-Automatico/practica3")
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
install.packages("RcppRoll")
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
install.packages("ddalpha")
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
install.packages("dimRed")
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
install.packages("gower")
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
summary(opt.tra)
describe(opt.tra)
labels = opt.tra[,"X0.26"]
colnames(opt.tra)
match(colnames(opt.tra),colnames(opt.tra))
colnames(opt.tra)!= "X0.26"
colnames(opt.tra)[colnames(opt.tra)!= "X0.26"]
View(opt.tra)
names.opt = colnames(opt.tra)
names.opt
match(names.opt[names.opt!="X0.26"], colnames(opt.tra))
predict.opt = opt.tra[,match(names.opt[names.opt!="X0.26"], colnames(opt.tra))]
predict.opt = opt.tra[,-match(names.opt[names.opt!="X0.26"], colnames(opt.tra))]
predict.opt = opt.tra[,-match(names.opt, colnames(opt.tra))]
predict.opt = opt.tra[,match(names.opt, colnames(opt.tra))]
predict.opt = opt.tra[,-match(names.opt, colnames(opt.tra))]
predict.opt = opt.tra[,-match(names.opt[names.opt!="X0.26"], colnames(opt.tra))]
predict.opt = opt.tra[,match(names.opt[names.opt!="X0.26"], colnames(opt.tra))]
cv.opt <- createFolds(labels, k = 10, returnTrain = TRUE)
colnames(cv.opt)
summary(cv.opt)
cv.opt[1]
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
cv.opt[1]
as.array(cv.opt[1])
cv.opt[1][1]
cv.opt[1][,2]
cv.opt[1][2]
cv.opt[1][1]
cv.opt[1][2,]
cv.opt[1][2]
cv.opt[1,2]
cv.opt[1,1]
cv.opt[1,]
cv.opt$Fold1
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
labels
labels(c(23,55,700))
labels[c(23,55,700)]
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
library(glmnet)
grid <- expand.grid(alpha = c(0,.1,.2,.3,.4,.5,.6,.7,.8,1),
lambda = seq(.01,.2, length.out = 20))
View(grid)
?trainControl
?train
predict.opt = opt.tra[,match(names.opt, colnames(opt.tra))]
library(caret)
library(glmnet)
#Lectura de los datos, Falta introducir la lectura del conjunto de test
opt.tra = read.csv("optdigits_tra.csv")
airfoil.tra = read.csv("airfoil_self_noise.csv")
#Configuracion de parémetros
seed = 674155357
set.seed(seed = seed)
numberOfPartes = 5
#Creacion de variables iniciales
#Describimos los datos del train
summary(opt.tra)
summary(airfoil.tra)
#Division de los datos en etiquetas y carasterísticas y iniciamos el cross validation
labels = opt.tra[,"X0.26"]
names.opt = colnames(opt.tra)
predict.opt = opt.tra[,match(names.opt, colnames(opt.tra))]
cv.opt <- createFolds(labels, k = numberOfPartes, returnTrain = TRUE)
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
View(glmnet_fit)
?train
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
source('E:/Projects/Aprendizaje-Automatico/practica3/Practica_3.R')
=======
}
}
iteration = iteration + 1
elem = elem +1
if(elem > length(Y)){elem = 1}
Historic = append(Historic,best)
}
df = data.frame( iterations = 1:iteration , Historic)
df
}
N <- 100
size <- 2
range = c( -size, size )
data <- simula_unif( N, rango = range )
f <- function(x){
sign(x[2]-x[1])
}
labels <- apply( data, FUN = f, MARGIN = 1 )
labels = noise(labels)
data <- cbind( data, 1 )
weights = c(0,0,0)
result = plot_error_PLA(data,labels,weights,0.01,length(labels))
ggplot(result,aes(y=Historic)) + geom_line(aes(x=iterations))
error_in <- function( data, labels, weights ){
sum <- 0
N <-length(labels)
for ( i in 1:N ){
h <- ((t(weights) %*% data[i,] ) * labels[i])
if(h > 0){
sum = sum +1
}
}
sum / N
}
plot_error_PLA <- function( X, Y, startWeights, learningRate = 0.01, maxIterations ){
i <- 0
threashold <- 0.01
ascending <- TRUE
weights <- startWeights
prevWeights <- weights
value = error_in(X,Y,weights)
Historic = c(value)
elem = 1
iteration = 1
best = value
while ( iteration < maxIterations & value < 1){
valor_predict = sign( as.numeric( t(weights) %*% X[elem,] ) )
wellClassified = valor_predict * Y[elem]
if ( wellClassified <= 0 ){
aux = weights
aux2 = best
weights = weights + learningRate * X[elem,] * Y[elem]
ascending <- TRUE
value = error_in(X,Y,weights)
best = value
if(aux2 > value){
weights = aux
best = aux2
}
}
iteration = iteration + 1
elem = elem +1
if(elem > length(Y)){elem = 1}
Historic = append(Historic,best)
}
df = data.frame( iterations = 1:iteration , Historic)
df
}
N <- 100
size <- 2
range = c( -size, size )
data <- simula_unif( N, rango = range )
f <- function(x){
sign(x[2]-x[1])
}
labels <- apply( data, FUN = f, MARGIN = 1 )
labels = noise(labels)
data <- cbind( data, 1 )
weights = c(0,0,0)
result = plot_error_PLA(data,labels,weights,0.01,1000)
ggplot(result,aes(y=Historic)) + geom_line(aes(x=iterations))
N <- 100
size <- 2
range = c( -size, size )
data <- simula_unif( N, rango = range )
lineX <- simula_unif( 2, dims = 1, rango = range )
lineY <- simula_unif( 2, dims = 1, rango = range )
f <- function(x){
( lineY[2] - lineY[1] ) / ( lineX[2] - lineX[1] ) * ( x - lineX[1] ) / lineX[2]
}
labels <- apply( data, FUN = f, MARGIN = 1 )
data <- cbind( data, 1 )
LR <- function( X, y, learningRate = 0.05, t = 0.1, itera = 1/t){
N <- length(y)
T <- as.integer(N * t)
w <- rnorm(dim(X)[2])
positive_exa <-which(y==1)
negative_exa <-which(y==-1)
N_po <- length(positive_exa)
N_ne <- length(negative_exa)
w_pre = w
scored = as.numeric(error_in(X,y,w))
for (a in 1:itera) {
g <- 0
for ( i in 1:T ){
pos <- ifelse((i %% 2) > 0,sample(N_po,1),sample(N_ne,1))
pos <- ifelse((i %% 2) > 0,positive_exa[pos],negative_exa[pos])
h <- t(w) %*% X[pos,]
er <- (h * y[pos])
g = g + (   ((X[pos,] * y[pos])*-1)/(1 + exp(er))   )
}
w <- w - learningRate * (g/N)
w <- as.vector(w)
}
w <- w /max(abs(w))
w
}
sigmoid <- function(x){
1 / (1 + exp(-x))
}
PlotRoc<-function(X,Y,w){
pred = (X%*% w)
Prediction = sigmoid(pred)
Sensibilidad = 1:100
Expecifidad = 1:100
for (i in 1:100) {
Prediction_pos = Y[which(Prediction >= i/100)]
Prediction_neg = Y[which(Prediction < i/100)]
VP = Prediction_pos[Prediction_pos > 0]
FP = Prediction_pos[Prediction_pos < 0]
VN = Prediction_neg[Prediction_neg < 0]
FN = Prediction_neg[Prediction_neg > 0]
Sensibilidad[i] = length(VP) / (length(VP) + length(FN))
Expecifidad[i] = 1 - (length(VN) / (length(VN) + length(FP)))
}
Expecifidad = 1 - Expecifidad
Sensibilidad = Sensibilidad[length(Sensibilidad):1]
df = data.frame(Sensibilidad, Expecifidad)
df
}
weights <- LR( data, labels )
N <- 100
size <- 2
range = c( -size, size )
data <- simula_unif( N, rango = range )
lineX <- simula_unif( 2, dims = 1, rango = range )
lineY <- simula_unif( 2, dims = 1, rango = range )
f <- function(x){
3*x[1] - x[2]
}
labels <- apply( data, FUN = f, MARGIN = 1 )
data <- cbind( data, 1 )
N <- 100
size <- 2
range = c( -size, size )
data <- simula_unif( N, rango = range )
lineX <- simula_unif( 2, dims = 1, rango = range )
lineY <- simula_unif( 2, dims = 1, rango = range )
f <- function(x){
sign(3*x[1] - x[2])
}
labels <- apply( data, FUN = f, MARGIN = 1 )
data <- cbind( data, 1 )
sigmoid <- function(x){
1 / (1 + exp(-x))
}
PlotRoc<-function(X,Y,w){
pred = (X%*% w)
Prediction = sigmoid(pred)
Sensibilidad = 1:100
Expecifidad = 1:100
for (i in 1:100) {
Prediction_pos = Y[which(Prediction >= i/100)]
Prediction_neg = Y[which(Prediction < i/100)]
VP = Prediction_pos[Prediction_pos > 0]
FP = Prediction_pos[Prediction_pos < 0]
VN = Prediction_neg[Prediction_neg < 0]
FN = Prediction_neg[Prediction_neg > 0]
Sensibilidad[i] = length(VP) / (length(VP) + length(FN))
Expecifidad[i] = 1 - (length(VN) / (length(VN) + length(FP)))
}
Expecifidad = 1 - Expecifidad
Sensibilidad = Sensibilidad[length(Sensibilidad):1]
df = data.frame(Sensibilidad, Expecifidad)
df
}
weights <- LR( data, labels )
result = PlotRoc(data,labels,-weights)
result2 = PlotRoc(data,labels,weights)
ggplot(result,aes(x=Expecifidad))+geom_line(aes(y=Sensibilidad, color="Inverse")) + geom_line(aes(y=result2$Sensibilidad, color="Regression Logistic")) + geom_line(aes(y=Expecifidad))
LR <- function( X, y, learningRate = 0.05, t = 0.1, itera = 1/t){
N <- length(y)
T <- as.integer(N * t)
w <- rnorm(dim(X)[2])
positive_exa <-which(y==1)
negative_exa <-which(y==-1)
N_po <- length(positive_exa)
N_ne <- length(negative_exa)
w_pre = w
scored = as.numeric(error_in(X,y,w))
for (a in 1:itera) {
g <- 0
for ( i in 1:T ){
pos <- ifelse((i %% 2) > 0,sample(N_po,1),sample(N_ne,1))
pos <- ifelse((i %% 2) > 0,positive_exa[pos],negative_exa[pos])
h <- t(w) %*% X[pos,]
er <- (h * y[pos])
g = g + (   ((X[pos,] * y[pos])*-1)/(1 + exp(er))   )
}
w <- w - learningRate * (g/N)
w <- as.vector(w)
}
w <- w /max(abs(w))
w
}
N <- 100
size <- 2
range = c( -size, size )
data <- simula_unif( N, rango = range )
lineX <- simula_unif( 2, dims = 1, rango = range )
lineY <- simula_unif( 2, dims = 1, rango = range )
f <- function(x){
sign(3*x[1] - x[2])
}
labels <- apply( data, FUN = f, MARGIN = 1 )
data <- cbind( data, 1 )
sigmoid <- function(x){
1 / (1 + exp(-x))
}
PlotRoc<-function(X,Y,w){
pred = (X%*% w)
Prediction = sigmoid(pred)
Sensibilidad = 1:100
Expecifidad = 1:100
for (i in 1:100) {
Prediction_pos = Y[which(Prediction >= i/100)]
Prediction_neg = Y[which(Prediction < i/100)]
VP = Prediction_pos[Prediction_pos > 0]
FP = Prediction_pos[Prediction_pos < 0]
VN = Prediction_neg[Prediction_neg < 0]
FN = Prediction_neg[Prediction_neg > 0]
Sensibilidad[i] = length(VP) / (length(VP) + length(FN))
Expecifidad[i] = 1 - (length(VN) / (length(VN) + length(FP)))
}
Expecifidad = 1 - Expecifidad
Sensibilidad = Sensibilidad[length(Sensibilidad):1]
df = data.frame(Sensibilidad, Expecifidad)
df
}
weights <- LR( data, labels )
result = PlotRoc(data,labels,-weights)
result2 = PlotRoc(data,labels,weights)
ggplot(result,aes(x=Expecifidad))+geom_line(aes(y=Sensibilidad, color="Inverse")) + geom_line(aes(y=result2$Sensibilidad, color="Regression Logistic")) + geom_line(aes(y=Expecifidad))
sigmoid <- function(x){
1 / (1 + exp(-x))
}
PlotRoc<-function(X,Y,w){
pred = (X%*% w)
Prediction = sigmoid(pred)
Sensibilidad = 1:100
Expecifidad = 1:100
for (i in 1:100) {
Prediction_pos = Y[which(Prediction >= i/100)]
Prediction_neg = Y[which(Prediction < i/100)]
VP = Prediction_pos[Prediction_pos > 0]
FP = Prediction_pos[Prediction_pos < 0]
VN = Prediction_neg[Prediction_neg < 0]
FN = Prediction_neg[Prediction_neg > 0]
Sensibilidad[i] = length(VP) / (length(VP) + length(FN))
Expecifidad[i] = 1 - (length(VN) / (length(VN) + length(FP)))
}
Expecifidad = 1 - Expecifidad
Sensibilidad = Sensibilidad[length(Sensibilidad):1]
df = data.frame(Sensibilidad, Expecifidad)
df
}
weights <- LR( data, labels )
result = PlotRoc(data,labels,-weights)
result2 = PlotRoc(data,labels,weights)
ggplot(result,aes(x=Expecifidad)) + geom_line(aes(y=result2$Sensibilidad, color="Regression Logistic")) + geom_line(aes(y=Expecifidad))
N <- 1000
dataTest <- simula_unif( N, rango = range )
labels <- apply( dataTest, FUN = f, MARGIN = 1 )
dataTest <- cbind( dataTest, 1 )
print(error_in(dataTest,labels,weights))
datos = c(1.68,0.18,0,0.27,0,0,0.000000023,0,0,0,0,0,0,0.000000023,0)
iter = 1:15000
df = as.data.frame(datos,iter)
iter = 1:length(datos)
df = as.data.frame(datos,iter)
df = as.data.frame(datos,iter,c("diferencia","iteraciones"))
plot(iter,datos)
?plot
plot(iter,datos,type = "l")
datos = c(1.7046,0.000018,1.10,0,0,0,0.0000033,0.00513,0.0000014,0,0,0.00000005,0.00000008,0.000000023,0)
plot(iter,datos,type = "l")
setwd("~/Documentos/Proyectos/AA/practica3")
#setwd("~/AA/2017/R")
knitr::opts_knit$set(root.dir = getwd())
#install.packages("ISLR")
library("ISLR", lib.loc="C:/Program Files/R/R-3.4.4/library")
#install.packages("ISLR")
library("ISLR")
data("Auto")   # loads the dataset
class(Auto)
colnames(Auto)
head(Auto)
summary(Auto)
dim(Auto)
plot(Auto$cylinders,Auto$mpg, main=" cylinders vs mpg")
boxplot(mpg~weight, data=Auto, xlab="weight", ylab = "mpg")
boxplot(mpg~cut(horsepower, breaks = 10),data = Auto)
attach ( Auto )  # para simplificar y prescindir del prefijo Auto
#pairs(~ .,data = Auto)  # todos con todos
pairs(~ mpg + cylinders + displacement + horsepower + weight + acceleration + year + origin, data= Auto)
# solo algunas
set.seed(1)
train = sample (nrow(Auto), round(nrow(Auto)*0.7)) # nos quedamos con los indices para el training
auto.train = Auto[train,]  # podemos reservarlos aparte ... con subset no serÃ????a necesario
auto.test = Auto[-train,]
ctrol = trainControl(method="cv", number=10)
library(caret)
ctrol = trainControl(method="cv", number=10)
set.seed(13)
lm05 = train(x= , y= ,method ="lm", trControl = ctrol)
library(caret)
ctrol = trainControl(method="cv", number=10)
set.seed(13)
lm05 = train(x= auto.train , y= ,method ="lm", trControl = ctrol)
View(auto.train)
auto.train = auto.train[,1:length(auto.train[,1])-1]
auto.train[,1]
auto.train[,2]
auto.train[1,]
auto.train[1:length(auto.train[1,]),]
auto.train[,1:length(auto.train[1,])]
auto.train[,1:length(auto.train[1,])-1]
auto.train = auto.train[,1:length(auto.train[1,])-1]
X = auto.train[,length(auto.train[1,])]
Y = auto.train[,length(auto.train[1,])]
X = auto.train[,1:length(auto.train[1,])-1]
library(caret)
ctrol = trainControl(method="cv", number=10)
set.seed(13)
lm05 = train(x= X , y= Y,method ="lm", trControl = ctrol)
library("MASS")
rml1 = rlm(mpg01 ~ weight + horsepower + displacement, data = Auto2, subset=train)
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
install.packages("MASS")
library("MASS")
#setwd("~/AA/2017/R")
knitr::opts_knit$set(root.dir = getwd())
#library("MASS")
rml1 = rlm(mpg01 ~ weight + horsepower + displacement, data = Auto2, subset=train)
#library(MASS)
library(MASS)
library("MASS", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
install.packages(c("amelie", "arules", "arulesViz", "bindr", "bindrcpp", "blob", "broom", "caret", "commonmark", "curl", "data.table", "DBI", "ddalpha", "deldir", "dendextend", "desc", "earth", "Formula", "gam", "gdtools", "h2o", "hms", "htmlwidgets", "httpuv", "igraph", "kernlab", "klaR", "ks", "lava", "LearnBayes", "lme4", "lmtest", "lubridate", "mapproj", "maps", "openssl", "party", "pillar", "plogr", "plotmo", "pROC", "prodlim", "proxy", "psych", "quantmod", "randomForest", "Rcpp", "rgl", "RMySQL", "robustbase", "R.oo", "RSQLite", "scatterplot3d", "sfsmisc", "slam", "sn", "sourcetools", "spam", "spData", "spdep", "sphet", "spls", "stringdist", "stringi", "tidyselect", "tinytex", "TSP", "viridis", "withr", "XML", "xts", "yaml"))
install.packages(c("amelie", "arules", "arulesViz", "bindr", "bindrcpp", "blob", "broom", "caret", "commonmark", "curl", "data.table", "DBI", "ddalpha", "deldir", "dendextend", "desc", "earth", "Formula", "gam", "gdtools", "h2o", "hms", "htmlwidgets", "httpuv", "igraph", "kernlab", "klaR", "ks", "lava", "LearnBayes", "lme4", "lmtest", "lubridate", "mapproj", "maps", "openssl", "party", "pillar", "plogr", "plotmo", "pROC", "prodlim", "proxy", "psych", "quantmod", "randomForest", "Rcpp", "rgl", "RMySQL", "robustbase", "R.oo", "RSQLite", "scatterplot3d", "sfsmisc", "slam", "sn", "sourcetools", "spam", "spData", "spdep", "sphet", "spls", "stringdist", "stringi", "tidyselect", "tinytex", "TSP", "viridis", "withr", "XML", "xts", "yaml"))
install.packages(c("amelie", "arules", "arulesViz", "bindr", "bindrcpp", "blob", "broom", "caret", "commonmark", "curl", "data.table", "DBI", "ddalpha", "deldir", "dendextend", "desc", "earth", "Formula", "gam", "gdtools", "h2o", "hms", "htmlwidgets", "httpuv", "igraph", "kernlab", "klaR", "ks", "lava", "LearnBayes", "lme4", "lmtest", "lubridate", "mapproj", "maps", "openssl", "party", "pillar", "plogr", "plotmo", "pROC", "prodlim", "proxy", "psych", "quantmod", "randomForest", "Rcpp", "rgl", "RMySQL", "robustbase", "R.oo", "RSQLite", "scatterplot3d", "sfsmisc", "slam", "sn", "sourcetools", "spam", "spData", "spdep", "sphet", "spls", "stringdist", "stringi", "tidyselect", "tinytex", "TSP", "viridis", "withr", "XML", "xts", "yaml"))
install.packages(c("amelie", "arules", "arulesViz", "bindr", "bindrcpp", "blob", "broom", "caret", "commonmark", "curl", "data.table", "DBI", "ddalpha", "deldir", "dendextend", "desc", "earth", "Formula", "gam", "gdtools", "h2o", "hms", "htmlwidgets", "httpuv", "igraph", "kernlab", "klaR", "ks", "lava", "LearnBayes", "lme4", "lmtest", "lubridate", "mapproj", "maps", "openssl", "party", "pillar", "plogr", "plotmo", "pROC", "prodlim", "proxy", "psych", "quantmod", "randomForest", "Rcpp", "rgl", "RMySQL", "robustbase", "R.oo", "RSQLite", "scatterplot3d", "sfsmisc", "slam", "sn", "sourcetools", "spam", "spData", "spdep", "sphet", "spls", "stringdist", "stringi", "tidyselect", "tinytex", "TSP", "viridis", "withr", "XML", "xts", "yaml"))
install.packages(c("amelie", "arules", "arulesViz", "bindr", "bindrcpp", "blob", "broom", "caret", "commonmark", "curl", "data.table", "DBI", "ddalpha", "deldir", "dendextend", "desc", "earth", "Formula", "gam", "gdtools", "h2o", "hms", "htmlwidgets", "httpuv", "igraph", "kernlab", "klaR", "ks", "lava", "LearnBayes", "lme4", "lmtest", "lubridate", "mapproj", "maps", "openssl", "party", "pillar", "plogr", "plotmo", "pROC", "prodlim", "proxy", "psych", "quantmod", "randomForest", "Rcpp", "rgl", "RMySQL", "robustbase", "R.oo", "RSQLite", "scatterplot3d", "sfsmisc", "slam", "sn", "sourcetools", "spam", "spData", "spdep", "sphet", "spls", "stringdist", "stringi", "tidyselect", "tinytex", "TSP", "viridis", "withr", "XML", "xts", "yaml"))
install.packages(c("amelie", "arules", "arulesViz", "bindr", "bindrcpp", "blob", "broom", "caret", "commonmark", "curl", "data.table", "DBI", "ddalpha", "deldir", "dendextend", "desc", "earth", "Formula", "gam", "gdtools", "h2o", "hms", "htmlwidgets", "httpuv", "igraph", "kernlab", "klaR", "ks", "lava", "LearnBayes", "lme4", "lmtest", "lubridate", "mapproj", "maps", "openssl", "party", "pillar", "plogr", "plotmo", "pROC", "prodlim", "proxy", "psych", "quantmod", "randomForest", "Rcpp", "rgl", "RMySQL", "robustbase", "R.oo", "RSQLite", "scatterplot3d", "sfsmisc", "slam", "sn", "sourcetools", "spam", "spData", "spdep", "sphet", "spls", "stringdist", "stringi", "tidyselect", "tinytex", "TSP", "viridis", "withr", "XML", "xts", "yaml"))
library("MASS", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
#setwd("~/AA/2017/R")
knitr::opts_knit$set(root.dir = getwd())
#library("MASS")
rml1 = rlm(mpg01 ~ weight + horsepower + displacement, data = Auto2, subset=train)
#library("MASS")
rml1 = rlm(mpg01 ~ weight + horsepower + displacement, data = Auto, subset=train)
#library("MASS")
rml1 = rlm(mpg ~ weight + horsepower + displacement, data = Auto, subset=train)
summary(rml1)
m1 = lm(mpg ~ weight, data=Auto, subset=train)
print(m1)
summary(m1)
plot(weight, mpg, main=" weight vs mpg")
m1 = lm(mpg ~ weight, data=Auto, subset=train)
print(m1)
summary(m1)
plot(Auto$weight, Auto$mpg, main=" weight vs mpg")
abline(m1$coefficients, col=2)
m2 = lm(mpg ~ horsepower, data=Auto, subset=train)
plot(horsepower, mpg, main=" horsepower vs mpg")
m2 = lm(mpg ~ horsepower, data=Auto, subset=train)
plot(Auto$horsepower, Auto$mpg, main=" horsepower vs mpg")
abline(m2$coefficients, col=2)
summary(m2)
m3 = lm(mpg ~ ., data=Auto, subset=train) # en funciÃ³n del resto, de TODOS
#coef(m3)
m4 = lm(mpg ~ weight + horsepower + displacement, data=Auto, subset=train)
summary(m4)
methods(class=class(m4))
m5 = lm(mpg ~ I(weight^2), data=Auto, subset=train)
coef(m5)
plot(mpg~weight)
m5 = lm(mpg ~ I(weight^2), data=Auto, subset=train)
coef(m5)
plot(Auto$mpg~Auto$weight)
w= m5$coefficients
x = matrix(rep(1, length(Auto$weight)),nrow= length(Auto$weight))
x= cbind (x, Auto$weight^2)
y= apply(x, 1, function(vec) w %*% vec)
points(Auto$weight, y, col=2)
yhatm1Tr = predict(m1) # usa el propio training
yhatm1Tst = predict(m1, auto.test, type= "response")
etr = mean((yhatm1Tr - auto.train[,1])^2)
etst = mean((yhatm1Tst - auto.test[,1])^2)
install.packages("leaps")
library("leaps")
Auto3 = Auto[,-ncol(Auto)]
subset.exh = regsubsets ( Auto3$mpg ~. , data = Auto3, nvmax = 8, method="exhaustive")
summary ( subset.exh )
subset.fwd = regsubsets ( Auto3$mpg ~. , data = Auto3 , nvmax =8 ,method ="forward")
reg.summary = summary ( subset.fwd )
#par ( mfrow = c (1 ,2) )
plot ( reg.summary$cp , xlab =" Number of Variables " , ylab =" Cp " ,
type = "l")
which.min (reg.summary$cp )
plot (reg.summary$bic , xlab =" Number of Variables " , ylab =" BIC " ,
type = "l")
which.min ( reg.summary$bic )
Auto2 = data.frame(mpg01 = (ifelse(mpg<median(mpg),0,1)),Auto)
Auto2 = data.frame(mpg01 = (ifelse(Auto$mpg<median(mpg),0,1)),Auto)
Auto2 = data.frame(mpg01 = (ifelse(Auto$mpg<median(Auto$mpg),0,1)),Auto)
ml1 = glm(mpg01 ~ weight + horsepower + displacement,
family = binomial(logit), data = Auto2, subset=train)
summary(ml1)
#Calculo de probabilidades
probTr.ml1 = predict(ml1, type="response")
probTstml1 = predict(ml1, data.frame(Auto2[-train,-1]), type="response")
predTstml1 = rep(0, length(probTstml1))  # predicciones por defecto 0
predTstml1[probTstml1 >=0.5] = 1          # >= 0.5 clase 1
table(predTstml1, Auto2[-train,1])  # para el calculo del Eval
Eval = mean(predTstml1 != Auto2[-train,1])
cat("Eval con el modelo LR "); print(ml1$call)
print(Eval)
ml2 = glm(mpg01 ~ weight + horsepower + displacement,
family = gaussian(identity), data = Auto2, subset=train)
summary(ml2)
install.packages("ROCR")
library("ROCR")
pred1 = prediction(probTstml1,Auto2[-train,1])
perf1 = performance(pred1,"tpr","fpr")
plot(perf1, main="conjunto de test")#,add=TRUE) # pinta la curva
probTstml2 = predict(ml2, data.frame(Auto2[-train,-1]), type="response")
pred2 = prediction(probTstml2,Auto2[-train,1])
perf2 = performance(pred2,"tpr","fpr")
legend(0.6,0.6,c("ML1","ML2"),col=c(2,3),lwd=3)
probTstml2 = predict(ml2, data.frame(Auto2[-train,-1]), type="response")
pred2 = prediction(probTstml2,Auto2[-train,1])
perf2 = performance(pred2,"tpr","fpr")
plot(perf2,lty=3, col="blue",add=TRUE))
probTstml2 = predict(ml2, data.frame(Auto2[-train,-1]), type="response")
pred2 = prediction(probTstml2,Auto2[-train,1])
perf2 = performance(pred2,"tpr","fpr")
plot(perf2,lty=3, col="blue",add=TRUE)
probTstml2 = predict(ml2, data.frame(Auto2[-train,-1]), type="response")
pred2 = prediction(probTstml2,Auto2[-train,1])
perf2 = performance(pred2,"tpr","fpr")
plot(perf2,lty=3, col="blue")
legend(0.6,0.6,c("ML1","ML2"),col=c(2,3),lwd=3)
#plot(perf2,lty=3, col="blue",add=TRUE)) # pinta la curva
library("AppliedPredictiveModeling")
install.packages("AppliedPredictiveModeling")
library("AppliedPredictiveModeling")
library(help=AppliedPredictiveModeling)
data("segmentationOriginal")
class(segmentationOriginal)
names(segmentationOriginal)[1:10]
summary(segmentationOriginal[1:10])
cellcase = segmentationOriginal$Case
unique(cellcase)
segData.tr = subset(segmentationOriginal, Case == "Train")
dim(segData.tr)
dim(segmentationOriginal)
cellClass = segData.tr$Class
unique(cellClass)
cellID = segData.tr$Cell
length(unique(cellID))
segData.tr = segData.tr[, -c(1:3)]  # eliminadas los 3 primeras atributos
length(grep("Status", names(segData.tr)))
b = (grep("Status", names(segData.tr)))
segData.tr = segData.tr[,-b]
dim(segData.tr)
names(segData.tr)
par(mfrow=c(1,2))
hist (segData.tr$AreaCh1)
range(segData.tr$AreaCh1)
hist(segData.tr$AngleCh1)
range(segData.tr$AngleCh1)
par(mfrow=c(1,1))
library("e1071")
?skewness
skewness(segData.tr$AreaCh1)
skewness(segData.tr$AngleCh1)
v_asimetria = apply(segData.tr,2,skewness)
v_asimetria[1:15]
sort(abs(v_asimetria), decreasing = T)[1:10]
#install.packages("caret")
#library("caret", lib.loc="~/R/x86_64-redhat-linux-gnu-library/3.3")
library("caret")
library("caret", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
#install.packages("caret")
#library("caret", lib.loc="~/R/x86_64-redhat-linux-gnu-library/3.3")
#library("caret")
BoxCoxTrans(segData.tr$AngleCh1)  # no transformacion
par(mfrow=c(1,2))
hist (segData.tr$AreaCh1)
hist(predict(Ch1Area_trans,head(segData.tr$AreaCh1)))
>>>>>>> Stashed changes
